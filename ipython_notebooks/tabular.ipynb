{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import the necessary packages: \n",
    "* `matplotlib.pyplot` and `seaborn` for plots\n",
    "* `numpy`\n",
    "* `gym` is needed to build the environments\n",
    "* `tqdm` is nice\n",
    "* `rl_agents` necessary classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "# from rl_agents.runners import simple_tab_runner\n",
    "from rl_agents.agents import QLearningAgent, SarsaAgent, ExpectedSarsaAgent\n",
    "from rl_agents.agents.functions import QMatrixFunction\n",
    "from rl_agents.agents.policies import EGreedyPolicy, EDecreasePolicy, BoltzmanPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(6,4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_avg(vector):\n",
    "    output = np.zeros(vector.size)\n",
    "    total = 0\n",
    "    for ii, elem in enumerate(vector):\n",
    "        total += elem\n",
    "        output[ii] = total / (ii+1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the simulator.\n",
    "\n",
    "The code below is almost the same as in `rl_agents.runners.simple_tab_runner`, the only difference is the substitution of the `while` with a `for` to avoid a episode that runs for too long. We define it here just to show how can you create your own setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tab_runner(env, agent, n_episodes):\n",
    "    rewards = np.zeros(n_episodes)\n",
    "    for ii in tqdm(range(n_episodes)):\n",
    "        # Run episode:\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        for _ in range(200):\n",
    "            action = agent.predict(obs)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            agent.learn(obs, action, reward, next_obs)\n",
    "            agent.policy.update()\n",
    "            obs = next_obs\n",
    "            episode_reward += reward\n",
    "            if done: break\n",
    "        rewards[ii] = episode_reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation runner:\n",
    "We also define a runner for testing the agent. In this setting we activate the flag `eval` in the `agent.predict()` to activate a greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tab_runner_eval(env, agent, n_episodes):\n",
    "    rewards = np.zeros(n_episodes)\n",
    "    for ii in tqdm(range(n_episodes)):\n",
    "        # Run episode:\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        for _ in range(100):\n",
    "#             env.render()\n",
    "            action = agent.predict(obs, eval=True)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "#             agent.learn(obs, action, reward, next_obs)\n",
    "#             agent.policy.update()\n",
    "            obs = next_obs\n",
    "            episode_reward += reward\n",
    "            if done: break\n",
    "        rewards[ii] = episode_reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Just a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0')\n",
    "agentQ = QLearningAgent(n_states=env.nS, n_actions=env.nA, alpha=0.5, gamma=0.9, q_func_kwargs={\"method\":\"zeros\"})\n",
    "agentS = SarsaAgent(n_states=env.nS, n_actions=env.nA, alpha=0.5, gamma=0.9, q_func_kwargs={\"method\":\"zeros\"})\n",
    "agentE = ExpectedSarsaAgent(n_states=env.nS, n_actions=env.nA, alpha=0.5, gamma=0.9, q_func_kwargs={\"method\":\"zeros\"})\n",
    "results = []\n",
    "for agent in [agentQ, agentS, agentE]:\n",
    "    env.seed(60)\n",
    "    np.random.seed(40)\n",
    "    results.append(simple_tab_runner(env, agent, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rewards, label in zip(results,[\"QLearning\", \"Sarsa\", \"Expected-Sarsa\"]):\n",
    "    plt.plot(cumulative_avg(rewards), label=label)\n",
    "plt.legend()\n",
    "plt.title('Total Reward by Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for agent in [agentQ, agentS, agentE]:\n",
    "    env.seed(123)\n",
    "    np.random.seed(457)\n",
    "    results.append(simple_tab_runner_eval(env, agent, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rewards, label in zip(results,[\"QLearning\", \"Sarsa\", \"Expected-Sarsa\"]):\n",
    "    plt.plot(cumulative_avg(rewards), label=label)\n",
    "plt.legend()\n",
    "plt.title('Total Reward by Episodes: Evaluating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:\n",
    "\n",
    "This is just one experiment, without a hyperparameter study or even a MonteCarlo simulation, so we can just take small conclusions, not really state anything, but we can see from comparing the training and evaluating plot that:\n",
    "\n",
    "* The Q-Learning has the worst performance in the training, with being Expected-Sarsa being the best option: Since Q-Learning takes a greedy policy in the TD update for the next state, while Sarsa and Expected-Sarsa take in consideration the exploration policy, they  perform better at this stage.\n",
    "* The Q-Learning performs better than both Expected-Sarsa and Sarsa in the evaluating stage, where the policy is changed into a greedy policy. Since both Sarsa and Expected-Sarsa takes into consideration the exploration policy when learning, when the policy changes to greedy, it decreases their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A hyperparameter study\n",
    "\n",
    "In this initial study, we want to find the best alpha and gamma for each algorithm.\n",
    "We will use a epsilon-decreasing policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = EDecreasePolicy(0.9, 0.01, 0.995)\n",
    "epsilons = []\n",
    "for _ in range(1000):\n",
    "    policy.update()\n",
    "    epsilons.append(policy.epsilon)\n",
    "plt.plot(epsilons)\n",
    "plt.title(\"Epsilon Decrease\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(env, Agent, alphas, gammas, methods, n_episodes):\n",
    "    n_montecarlo = 10\n",
    "    output = {}\n",
    "    param_list = [(alpha, gamma, method) for alpha in alphas for gamma in gammas for method in methods]\n",
    "    for alpha, gamma, method in tqdm(param_list):\n",
    "        total_mc_reward = 0\n",
    "        for mc in range(n_montecarlo): # Monte carlo loop:\n",
    "            # We instantiate the agent inside the monte carlo loop due to the \"random\" initialization method and to reinitiate the epsilon \n",
    "            agent = Agent(n_states=env.nS, n_actions=env.nA, alpha=alpha, gamma=gamma, policy=EDecreasePolicy(0.9, 0.01, 0.99), q_func_kwargs={\"method\":method})\n",
    "            total_reward = 0\n",
    "            # This garantees that each parameter combination experiences\n",
    "            # the same conditions for the same monte carlo run.\n",
    "            np.random.seed(mc*123)\n",
    "            env.seed(mc*456)\n",
    "            for ii in range(n_episodes):\n",
    "                # Run episode:\n",
    "                obs = env.reset()\n",
    "                done = False\n",
    "                for _ in range(100):\n",
    "                    action = agent.predict(obs)\n",
    "                    next_obs, reward, done, info = env.step(action)\n",
    "                    agent.learn(obs, action, reward, next_obs)\n",
    "                    agent.policy.update()\n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "                    if done: break\n",
    "            total_mc_reward += total_reward/n_episodes\n",
    "        output[(alpha, gamma, method)] = total_mc_reward/n_montecarlo\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.1, 0.9, 5)\n",
    "gammas = np.linspace(0.1, 0.9, 5)\n",
    "methods = [\"zeros\", \"ones\", \"random\"]\n",
    "n_episodes = 500\n",
    "ql_dict = grid_search(env, QLearningAgent, alphas, gammas, methods, n_episodes)\n",
    "sarsa_dict = grid_search(env, SarsaAgent, alphas, gammas, methods, n_episodes)\n",
    "esarsa_dict = grid_search(env, ExpectedSarsaAgent, alphas, gammas, methods, n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best combination for Q-learning:\", max(ql_dict, key=ql_dict.get), \"Value:\", ql_dict[max(ql_dict, key=ql_dict.get)])\n",
    "print(\"Best combination for Sarsa:\", max(sarsa_dict, key=sarsa_dict.get), \"Value:\", sarsa_dict[max(sarsa_dict, key=sarsa_dict.get)])\n",
    "print(\"Best combination for Expected-Sarsa:\", max(esarsa_dict, key=esarsa_dict.get), \"Value\", esarsa_dict[max(esarsa_dict, key=esarsa_dict.get)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparison of the basic tabular methods.\n",
    "\n",
    "Now that we have foumd the best hyperparameters, we can compare the different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, gamma, method = max(ql_dict, key=ql_dict.get)\n",
    "agentQ = QLearningAgent(n_states=env.nS, n_actions=env.nA, alpha=alpha, gamma=gamma, policy=EDecreasePolicy(0.9, 0.01, 0.995), q_func_kwargs={\"method\":method})\n",
    "alpha, gamma, method = max(sarsa_dict, key=sarsa_dict.get)\n",
    "agentS = SarsaAgent(n_states=env.nS, n_actions=env.nA, alpha=alpha, gamma=gamma, policy=EDecreasePolicy(0.9, 0.01, 0.995), q_func_kwargs={\"method\":method})\n",
    "alpha, gamma, method = max(esarsa_dict, key=esarsa_dict.get)\n",
    "agentE = ExpectedSarsaAgent(n_states=env.nS, n_actions=env.nA, alpha=alpha, gamma=gamma, policy=EDecreasePolicy(0.9, 0.01, 0.995), q_func_kwargs={\"method\":method})\n",
    "results = []\n",
    "for agent in [agentQ, agentS, agentE]:\n",
    "    env.seed(789)\n",
    "    np.random.seed(159)\n",
    "    results.append(simple_tab_runner(env, agent, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rewards, label in zip(results,[\"QLearning\", \"Sarsa\", \"Expected-Sarsa\"]):\n",
    "    plt.plot(cumulative_avg(rewards), label=label)\n",
    "plt.legend()\n",
    "plt.title('Total Reward by Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for agent in [agentQ, agentS, agentE]:\n",
    "    env.seed(123)\n",
    "    np.random.seed(457)\n",
    "    results.append(simple_tab_runner_eval(env, agent, 10))\n",
    "for rewards, label in zip(results,[\"QLearning\", \"Sarsa\", \"Expected-Sarsa\"]):\n",
    "    plt.plot(cumulative_avg(rewards), label=label)\n",
    "plt.legend()\n",
    "plt.title('Total Reward by Episodes: Evaluating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
